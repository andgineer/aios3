{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"reference/","title":"Reference","text":""},{"location":"reference/#aios3.file","title":"aios3.file","text":"<p>S3 \"files\" operations with aiobotocore.</p>"},{"location":"reference/#aios3.file-classes","title":"Classes","text":""},{"location":"reference/#aios3.file-functions","title":"Functions","text":""},{"location":"reference/#aios3.file.chunks","title":"aios3.file.chunks  <code>async</code>","text":"<pre><code>chunks(bucket: str, key: str, chunk_size: int | None = None, s3: AioBaseClient | None = None) -&gt; AsyncGenerator[bytearray, None]\n</code></pre> <p>Generate file chunks <code>chunk_size</code> bytes max.</p> <p>Args:     bucket: S3 bucket.     key: path in the bucket including \"file name\".     chunk_size: max number of bytes to read in one chunk. by default read file as one chunk.     s3: boto s3 client. by default it auto-created inside the function.</p> <p>Return:     Chunks of the file.</p> <p>Raises:     FileNotFoundError: If the S3 object does not exist.     ClientError: If other S3 operation fails.</p>"},{"location":"reference/#aios3.file.read","title":"aios3.file.read  <code>async</code>","text":"<pre><code>read(bucket: str, key: str, chunk_size: int | None = None, s3: AioBaseClient | None = None) -&gt; bytes\n</code></pre> <p>Read the full content of the file as bytes.</p> <p>Args:     bucket: S3 bucket.     key: path in the bucket including \"file name\".     chunk_size: max number of bytes to read in one chunk. by default read all.     s3: boto s3 client. by default it auto-created inside the function.</p> <p>Return:     The file content as bytes.</p> <p>Raises:     FileNotFoundError: If the S3 object does not exist.     ClientError: If other S3 operation fails.</p>"},{"location":"reference/#aios3.file.save","title":"aios3.file.save  <code>async</code>","text":"<pre><code>save(bucket: str, key: str, body: bytes, s3: AioBaseClient | None = None) -&gt; None\n</code></pre> <p>Create the file with the <code>body</code>.</p> <p>Args:     bucket: S3 bucket.     key: path in the bucket including \"file name\".     body: content to write into file.     s3: boto s3 client. by default it auto-created inside the function.</p> <p>Raises:     ClientError: If S3 operation fails.</p>"},{"location":"reference/#aios3.file.stream","title":"aios3.file.stream  <code>async</code>","text":"<pre><code>stream(bucket: str, key: str, chunk_size: int | None = None, s3: AioBaseClient | None = None) -&gt; IO[bytes]\n</code></pre> <p>Create file-like object to stream the file content.</p> <p>Note: This function loads all chunks into memory to create a synchronous stream. For true streaming without loading all data into memory, use the chunks() function directly.</p> <p>Args:     bucket: S3 bucket.     key: path in the bucket including \"file name\".     chunk_size: max number of bytes to read in one chunk. by default read file as one chunk.     s3: boto s3 client. by default it auto-created inside the function.</p> <p>Return:     Python file stream with the file content.</p> <p>Raises:     FileNotFoundError: If the S3 object does not exist.     ClientError: If other S3 operation fails.</p>"}]}